\documentclass{article}

\title{Egzamin RPS - ściąga}
\author{FS,FS,KS,PM,SM}

\usepackage{amsmath, amscd, amsthm, amssymb, mathrsfs, amsfonts}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{tikz}
\usepackage{mathtools}
\usepackage{polski}
\usepackage{gensymb}
\usepackage{tabu}
\usepackage{multicol}
\usepackage{wrapfig}
\usepackage[utf8]{inputenc}
\usepackage[fontsize=8pt]{fontsize}
\usepackage[compact]{titlesec}

\usetikzlibrary{quotes,angles}

\newcommand{\ub}[1]{\underline{\textbf{#1}}}
\newcommand{\lmi}[2][\infty]{\displaystyle{\lim_{#2 \to #1}}}
\newcommand{\nn}{\noindent\newline}

\begin{document}
    \noindent
    \section{Prawdopodobieństwo całkowite i warunkowe}
    Prawdopodobieństwo całkowite. Niech będzie dana przestrzeń probabilistyczna $(\Omega, \Sigma, P)$ oraz zdarzenia $A_1, A_2, A_n \in \Sigma$ spełniająca warunki: 
        $P(A_i) > 0$ dla każdego $i = 1,...,n$;
        $A_i \cap A_j = \emptyset$ dla wszystkich $i \neq j$;
        $A_1 \cup ... A_n = \Omega$
        \nn
    Prawdopodobieństwo warunkowe: $P(B|A) = \frac{P(B \cap A)}{P(A)}$
    \nn
    Wtedy dla każdego zdarzenia $B \in \Sigma$ zachodzi następująca równość: $P(B) = \Sigma_{i=1}^n P(B|A_i)P(A_i)$
    \nn
    Wzór Bayesa: $P(A_k|B) = \frac{P(B|A_k)P(A_k)}{\Sigma_{i=1}^n P(B|A_i)P(A_i)}$
    \nn
    Niezależność zdarzeń: $P(A \cap B) = P(A)\cdot P(B)$, $P(A_{k_1} \cap ... \cap A_{k_r}) = P(A_{k_1}) \cdot ... \cdot P(A_{k_r})$
    \section{Wartość oczekiwana i wariancja}
    Wartość oczekiwana dla rozkładu dyskretnego: $m = E(X) = \Sigma_{i=1}^n x_i p_i$, ciągłego: $m = E(X) = \int^{\infty}_{-\infty}x f(x) dx$
    \nn
    Wariancja: $\sigma^2 = D^2(X) = E((X-m)^2)$, odchylenie standardowe: $\sigma = \sqrt{\sigma^2} = \sqrt{D^2(X)}$
    \nn
    Wariancja dla rozkładu dyskretnego: $D^2(X) = \Sigma_{i=1}^n (x_i-m)^2 p_i$, dla rozkładu ciągłego: $D^2(X) = \int_{-\infty}^{\infty} (x-m)^2 f(x) dx$
    \nn
    Zmienne niezależne gdy dla dowolnych zdarzeń $B_1,...,B_k \in \Sigma$: $P(X_1 \in B_1, ..., X_k \in B_k) = P(X_1 \in B_1) \cdot ... \cdot P(X_k \in B_k)$
    \nn
    Wartości własności i wariancji: \nn
    jeżeli $X = const = c$, to $E(X) = c$;\quad 
    $E(aX) = aE(X) \forall a \in \mathbb{R}$;\quad
    $E(X+Y) = E(X)+E(Y)$;\quad
    $D^2(X) = E(X^2)-E(X)^2$;\quad
    $D^2(aX) = a^2D^2(X) \forall a \in \mathbb{R}$;\quad
    $X = const = c$ to $D^2(X) = 0$; 
    \nn
    jeżeli X i Y są niezależnymi zmiennymi losowymi, to $D^2(X+Y)=D^2(X)+D^2(Y)$

    \section{Rozkłady}
    Rozkład Bernouliego: ${n\choose{k}} p^k (1-p)^{n-k}$, $m = np$, $\sigma^2 = np(1-p)$
    \nn
    Jeżeli $X \sim B(n,p)$ i $Y \sim B(m,p)$ są dwiema niezależnymi zmiennymi losowymi o rozkładzie dwumianowym, wtedy ich suma $X+Y$ jest zmienną losową o rozkładzie dwumianowym $B(n+m, p)$
    \nn
    Rozkład Poissona : $f(x)= \frac{e^{-\lambda} \lambda^k}{k!}$, $m = \lambda$, $\sigma^2 = \lambda$\
    \nn
    Dla $n \geq 100 \land p \leq \frac{1}{10}$ rozkład Poissona z $\lambda = np$ dobrze przybliza rozkład Bernouliego
    \nn
    Dla dwóch zmiennych losowych o rozkładzie Poissona z parametrami $\lambda$ i $\mu$ suma tych zmiennych losowych ma rozkład Possiona o parametrze $\lambda+\mu$
    \nn
    Rozkład geometryczny: $P(k) = p(1-p)^{k-1}$, $m = \frac{1}{p}$, $\sigma^2 = \frac{1-p}{p^2}$ 
    \nn
    Rozkład jednostajny: $f(x) = \frac{1}{b-a}$ gdy $x \in [a,b]$, 0 gdy $x \notin [a,b]$, $ F(x)=$ 0 gdy $x < a$, $ \frac{x-a}{b-a}$ gdy $x \in [a,b]$, 1 gdy $x > b$, $m = \frac{a+b}{2}$, $\sigma^2 = \frac{(b-a)^2}{12} $
    \nn
    Rozkład wykładniczy: $f(x) = \lambda e^{-\lambda x}$, $F(x)= 1-e^{-\lambda x}$, $m = \frac{1}{\lambda}$, $\sigma^2 = \frac{1}{\lambda^2}$
    \nn
    Rozkład normalny: $f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{{(x-m)}^2}{2\sigma^2}}$  
    \nn
    Własności $\Phi(x)$: $\Phi(x) = 1 - \Phi(-x)$, $\Phi^{-1}(\alpha) = -\Phi^{-1}(1-\alpha)$, $x \in \mathbb{R}, \alpha \in [0,1]$
    \nn
    Dla X bedącego zmienną losową o rozkładzie normalnym $N(m,\sigma)$ i $Y = aX + b$, gdzie $a \neq 0$ Y ma rozkład normalny $N(am+b, |a|\sigma)$
    \\
    Dystrybuanta zmiennej losowej: $F(x) = F_X(x) = P_X((-\infty, x]) = P(X \in (-\infty, x])$. \\
    Pochodna dystrybuanty to funkcja rozkładu: $F'(x) = f(x)$\\
    Dystrybuanta jest niemalejąca, $\lim_{x \rightarrow -\infty}F(x) = 0$, $\lim_{x \rightarrow \infty}F(x) = 1$. \\
    Dla rozkładu dyskretnego: $F(x) = \sum_{i:x_i \le x}p_i$
    

    \section{Centralne twierdzenie graniczne}
	
    Dla $S_{n}= X_{1} + ... + X_{n}$, gdzie $X_{i}$ to niezależne zmienne losowe z tym samym rozkładem, nadzieją $m$ i wariancją $\sigma^{2}$, $\sigma>0$:\\ 
    $Z_{n} =\frac{S_{n} -E( S_{n})}{\sqrt{D^{2}( S_{n})}} =\frac{S_{n} -nm}{\sigma \sqrt{n}}$ - $Z_{n}$ to standaryzacja sumy $S_{n}$, $E(Z_{n})=0$, $D^{2}(Z_{n})=1$ \\
    tw. Lindeberga-Levy'ego: $\forall x\in \mathbb{R} $ $\lim _{n\rightarrow \infty } P( Z_{n} \ \leqslant x) =\Phi ( x)$ \\
    Centralne twierdzenie graniczne dla sum: $\forall x\in \mathbb{R} $ $\lim _{n\rightarrow \infty }( F_{S_{n}}( x) \ -\ \Phi _{nm,\sigma \sqrt{n}}( x)) =0$ \\
    Centralne twierdzenie graniczne dla średnich: $\forall x\in \mathbb{R} $ $\lim _{n\rightarrow \infty }( F_{\frac{S_{n}}{n}}( x) \ -\ \Phi _{m,\frac{\sigma }{\sqrt{n}}}( x)) =0$ \\
    tw. de Moivre’a-Laplace’a (gdy $X_{i}$ to ciąg niezależnych prób Bernoullego z tym samym $p$): $\forall x\in \mathbb{R} $ $P\left(\frac{S_{n} -np}{\sqrt{npq}} \leqslant x\right) \rightarrow \Phi ( x)$
    
    \section{Estymacja punktowa}
    Niech $X_1, ..., X_n$ będzie próbką prostą ze zmiennej losowej X. Estymatorem parametru $\theta$ rozkładu $P_\theta \in \mathbb{P}$ "odpowiednio bliskiego" rozkładowi $P_X$ nazywamy zmienną losową $\hat{\theta} \circ (X_1, ..., X_n) = T(X_1, ..., X_n)$ gdzie T jest odpowiednio dobraną funkcją, która "rozsądnie" przybliża (estymuje) wartość $\theta$.
    Przykładami estymatorów są: średnia arytmetyczna z próbki - $\bar{X} = \frac{X_1 + ... + X_n}{n}$ - estymator wartości oczekiwanej $E(X)$. \\ Mediana z próbki - $me X_{(\lceil n/2 \rceil)}$ - estymator mediany. Wariancja z próbki - $S^2 = \frac{1}{n}\sum^n_{i=1}(X_i - m)^2$ (jeżeli $E(X)=m$ jest znane), \\ lub $S^2 = \frac{1}{n} \sum^n_{i=1}
(X_i-\bar{X})^2$ (jeżeli $E(X)=m$ nie jest znane) - estymator wariancji $D^2(X)$ \\
    Estymator nieobciążony - $E(\hat{\theta}) = \theta$. \\ Estymator zgodny - $P(\omega \in \Omega : \lim_{n \rightarrow \infty} \hat{\theta}_n(\omega) = \theta) = 1$
     \nn
    Metoda MLE: dla zmiennych losowych $X_1, ..., X_n$ \nn
    $L(\theta) = \Pi_{i = 1}^{n} P(X_i = x_i)$ - dla zmiennych dyskretnych \nn
    $L(\theta) = \Pi_{i = 1}^{n} f(x_i)$ - dla zmiennych ciągłych \nn
    Żeby wyznaczyć MLE $(\theta)$ należy wyznaczyć maximum funkcji wiarygodności$L(\theta)$

    \section{Przedziały ufności Estymacja Przedziałowa}
    
    Dla wartości oczekiwanej w rozkładzie normalnym ze znanym odchyleniem standardowym (na poziomie ufności $1-\alpha$): \nn
    $(\bar{X} - \frac{\sigma}{\sqrt{n}} \Phi^{-1}(1-\frac{\alpha}{2}), \bar{X} + \frac{\sigma}{\sqrt{n}}\Phi^{-1}(1 - \frac{\alpha}{2}))$, 
    $(-\infty, \bar{X} + \frac{\sigma}{\sqrt{n}}\Phi^{-1}(1-\alpha))$,
    $(\bar{X} - \frac{\sigma}{\sqrt{n}}\Phi^{-1}(1-\alpha), \infty)$

    \noindent
    Dla wartości oczekiwanej w rozkładzie normalnym z nieznanym odchyleniem standardowym: \nn
    $(\bar{X} - \frac{S}{\sqrt{n-1}} F^{-1}_{t_{n-1}}(1-\frac{\alpha}{2}), \bar{X} + \frac{S}{\sqrt{n-1}} F^{-1}_{t_{n-1}}(1-\frac{\alpha}{2}))$,
    $(-\infty, \bar{X} + \frac{S}{\sqrt{n-1}} F^{-1}_{t_{n-1}}(1-\alpha))$,
    $(\bar{X} - \frac{S}{\sqrt{n-1}} F^{-1}_{t_{n-1}}(1-\alpha), \infty)$,

    \noindent
    Dla frakcji. Próbka prosta $X_1, ..., X_n$ pochodzi z rozkładu dwupunktowego $B(1,p)$. W przypadku. Dla próbki dużej $(n > 30)$: \nn
    $(\hat{p} - \frac{ \sqrt{ \hat{p} (1-\hat{p}) } }{\sqrt{n}} \Phi^{-1}(1-\frac{\alpha}{2}), 
    \hat{p} + \frac{ \sqrt{ \hat{p} (1-\hat{p}) } }{\sqrt{n}} \Phi^{-1}(1-\frac{\alpha}{2}))$ ,
    $(0, \hat{p} + \frac{ \sqrt{ \hat{p} (1-\hat{p}) } }{\sqrt{n}} \Phi^{-1}(1-\alpha)$ ,
    $(\hat{p} - \frac{ \sqrt{ \hat{p} (1-\hat{p}) } }{\sqrt{n}} \Phi^{-1}(1-\alpha), 1)$ gdzie $\hat{p} = \bar{X_n} = \frac{\#\{i:X_i=1\}}{n}$

    \noindent
    Dla wariancji w rozkładzie normalnym z nieznaną wartością oczekiwaną: 
    $( \frac{nS^2}{ F^{-1}_{\chi^2_{n-1}} (1-\frac{\alpha}{2}) }, \frac{nS^2}{F^{-1}_{\chi^2_{n-1}} (\frac{\alpha}{2})})$,
    $(0, \frac{nS^2}{F^{-1}_{\chi^2_{n-1}} (\alpha)})$, 
    $(\frac{nS^2}{F^{-1}_{\chi^2_{n-1}} (1-\alpha)}, \infty)$ 

    \noindent
    Przedziały ufności dla wartości oczekiwanej - uwagi. Jeżeli rodzina rozkładów nie jest znana oraz próbka jest duża $(n \ge 30)$, to konstruując przedziały ufności dla wartości oczekiwanej m możemy rozważyć zmienną losową $Z = \frac{\bar{X} - m}{S}\sqrt{n} \approx N(0,1)$ \\
    Jeżeli natomiast próbka jest mała $(n < 30)$ oraz pochodzi z rozkładu $B(1,p)$ to konstruując przedział ufności dla p możemy rozważyć zmienną losową $K = \#\{i : X_i = 1\} \sim B(n,p)$ 
    
    \section{Testowanie hipotez statystycznych}
    Próbka $X_{1},...,X_{n}$ z rozkładu $N(m,\sigma)$, stat. testowe dają zm. los. przy prawdziwości hipotez zerowych.\\ 
    Testowanie hipotez $H_{0}: m=m_{0}$ o wart. oczekiwanej w rozkładzie norm.: \\ gdy $\sigma$ znana $z=z(x_{1},...,x_{n})=\frac{\bar{x}-m_{0}}{\sigma}\sqrt{n}$ dająca zm. los. $Z=z(X_{1},...,X_{n})$ o rozkładzie $N(0,1)$, \\ gdy $\sigma$ nieznana $t=t(x_{1},...,x_{n})=\frac{\bar{x}-m_{0}}{s}\sqrt{n-1}$ dająca zm. los. $T=t(X_{1},...,X_{n})$ o rozkładzie t-studenta o $n-1$ st. swobody. \\
    Testowanie hipotez $H_{0}: \sigma^{2} = \sigma_{0}^{2}$ o wariancji w rozkładzie norm.: \\ $\chi=\chi(x_{1},...,x_{n})=\frac{ns^{2}}{\sigma_{0}^{2}}$ dająca zm. los. $\chi=\chi(X_{1},...,X_{n})$ o rozkładzie $\chi$ a o $n-1$ st. swobody. \\
    Testowanie hipotez $H_{0}: p=p_{0}$ o frakcji, \\ gdy $X_{1},...,X_{n}$ z rozkładu $B(1,p)$: dla próbki $n\geqslant30$ używamy stat. testowej $z=z(x_{1},...,x_{n})=\frac{\hat{p}-p_{0}}{\sqrt{\hat{p}(1-\hat{p})}}\sqrt{n}$ dająca zm. los. $Z=z(X_{1},...,X_{n})$ o rozkładzie $N(0,1)$, \\ dla małej próbki stat. testowa $k=k(x_{1},...,x_{n})=\#\{i:x_{i}=1\}$ dająca zm. los. $K=k(X_{1},...,X_{n})$ o rozkładzie $B(n,p_{0})$.\\
    Test t-Studenta: próbki z rozkł. $N(m_{1},\sigma_{1})$ i $N(m_{2},\sigma_{2})$,  $H_{0}: m_{1}=m_{2}$, \\ dla znanych $\sigma_{1}$,$\sigma_{2}$:  $z=z(x_{1},...,x_{n_{1}},y_{1},...,y_{n_{2}})=\frac{\hat{x}-\hat{y}}{\sqrt{\frac{\sigma_{1}^{2}}{n_{1}}+\frac{\sigma_{2}^{2}}{n_{2}}}})$ dająca zm. los. $Z$ o rozkł. $N(0,1)$, \\ dla nieznanych $\sigma_{1}$,$\sigma_{2}$: $t=t(x_{1},...,x_{n_{1}},y_{1},...,y_{n_{2}})=\frac{\hat{x}-\hat{y}}{\sqrt{\frac{n_{1}s_{1}^{2}+n_{2}s_{2}^{2}}{n_{1}+n_{2}-2}\cdot\frac{n_{1}+n_{2}}{n_{1}n_{2}}}})$ dająca zm. los. $T$ o rozkładzie t-studenta o $n_{1}+n_{2}-2$ st. swobody. 
    Test$\chi^2$ zgodnosci: dla rozkładów dyskretnych: $H_0 : P_X = P$ \\
    $P(y_1) = \pi_1 > 0$, ..., $P(y_k) = \pi_k > 0$, $\pi_1 +$, ... ,$+ \pi_k = 1$, $n_i$ - liczba wystąpień $y_i$ w ciągu $x1,...,x_n$ \\
    $\chi = \chi(x_1, ..., x_n) = \sum_{i=1}^{k} \frac{(n_i-n\pi_i)^2}{n\pi_i}$, zbiór krytyczny K to $[l,\inf)$, gdzie l to kwantyl rzędu $1-\alpha$ rozkładu $\chi^2$ o $k-1$ stopniach swobody. \\
    Test niezależności rozkładów: dla zmiennych losowych: $X \sim P_X$ i $Y \sim P_Y$ \\
    $\chi = \chi((x_1,y_1), ..., (x_n,y_n)) = \sum_{(s,t)\in S \times T} \frac{(n_{s,t}-\frac{n_s n_t}{n})^2}{\frac{n_s n_t}{n}}$, gdzie $ S \times T$ jest nośnikiem próbki danych, \\ $n_{s,t} = \#\{i:(s,t)=(x_i,y_i)\}$, $n_s = \#\{i: s=x_i\}$, $n_t = \#\{i: t=y_i\}$ $ (n=\sum_{(s,t)}n_{s,t}) $ Można wtedy wykazać, że $\chi \approx \chi_{(\# S-1)(\# T-1)}^2$

    \section{Metoda bootstrap}
    Dla małej próbki (o wielkości $n$) i nieznanym rozkładzie losujemy z niej ze zwracaniem kolejno $B$ próbek o wielkości $n$. \\
    Estymator bootstrapowy parametru ze znanym estymatorem $g(x_{1},...,x_{n})$ to: $\hat{g}=\hat{g}(x_{1},...,x_{n})=\frac{1}{B}\sum _{i=1}^{B} g(x^{i}_{1},...,x^{i}_{n})$, gdzie $x^{i}_{1},...,x^{i}_{n}$ to próbka wylosowana za $i$-tym razem. \\
     Metoda percentylowa wyznaczania przedziałów ufności parametru $\theta$: losujemy 1000 próbek bootstrapowych, dla każdej obliczamy estymator $\theta$. Kwantyle odpowiednich rzędów z ciągu estymatorów dla próbek są końcami przedziału ufności.


    \section{Wektor losowy}
    Wektor losowy: funkcja $X: \Omega \rightarrow \mathbb{R}^{n}$ ($Y: \Omega \rightarrow \mathbb{R}^{m}$) na przestrzeni $(\Omega,\Sigma,P)$, rozkład wektora losowego $X$: $P_{X}(B) = P(X^{-1}(B)$ dla $B \subset \mathbb{R}^{n}$. Dla $A_{1} \subset \mathbb{R}^{n} A_{2} \subset \mathbb{R}^{m}$: $P_{X}(A_{1}) = P_{(X,Y)}(A_{1} \times \mathbb{R}^{m})$ i $P_{Y}(A_{2}) = P_{(X,Y)}(\mathbb{R}^{n} \times A_{2})$ są rozkładami brzegowymi, a $P_{(X,Y)}$ to rozkład łączny.\\
    Niezależność wektorów losowych o rozkładach ciągłych $f_{(X,Y)}(x,y) = f_{X}(x) f_{Y}(y)$ \\
    dla $x \in \mathbb{R}^{n}, y \in \mathbb{R}^{m}, P_{X}(x)>0, P_{Y}(y)>0, f_{X}(x)>0, f_{Y}(y)>0$ \\
    Rozkłady warunkowe wektora losowego (dyskretny):  $P_{X|Y=y}(B) = P(X \in B | Y = y) = \frac{P(X \in B, Y = y)}{P(Y = y)}$ dla $B \subset \mathbb{R}^{n}$ \\
    Rozkłady warunkowe wektora losowego (ciągłego):  $f_{Y|X=x}(y) = \frac{f_{(X,Y)}(x,y)}{f_{X}(x)}$ dla $y \in \mathbb{R}^{m}$ \\
    Warunkowa wartość oczekiwana: $E(X|Y=y)$
    
    \section{Regresja Liniowa}
    Model regresji liniowej: $Y_{i}=\alpha + \beta x_{i} + U_{i}$ dla $i=1,..,n$, \\
    Wyznaczenie estymatorów $\alpha$ i $\beta$ MNK: wyznaczamy arg min $S(\alpha,\beta)$ dla$S(\alpha,\beta)$= $\Sigma_{i=1}^n (y_{i}-\alpha - \beta x_{i})^{2}$, \nn otrzymujemy $\hat{\alpha} = \bar{y}-\hat{\beta \bar{x}}$
    $\hat{\beta}=\frac{n\Sigma_{i=1}^n x_{i} y_{i} - (\Sigma_{i=1}^n x_{i})(\Sigma_{i=1}^n y_{i})}{n \Sigma_{i=1}^n x_{i}^2 - (\Sigma_{i=1}^n x_{i})^2}$ $\hat{\alpha}$ i $\hat{\beta}$ są nieobciążone.\\
    Wyznaczenie estymatorów metodą największej wiarygodności dla błędów normalnych: \\
    Zał: $U_i \sim N(0,\sigma)$, czyli $Y \sim N(\alpha+\beta x_{i},\sigma)$\\ $L(\alpha,\beta,\sigma^2)=f_1(y_1)\cdot\cdot\cdot f_n(y_n)$, gdzie $f_i(y)=\frac{1}{\sqrt{2 \pi \sigma^2}}e^{\frac{-(y- \alpha - \beta x_{i})^2}{(2 \sigma^2)}}$ dostajemy te same estymatory jak w MNK oraz $\hat{\sigma^2}=\frac{1}{n}
    \Sigma_{i=1}^n (y_i-\hat{\alpha}-\hat{\beta} x_i)^2$ a także $E(\hat{\sigma^2})=\frac{n-2}{n}{\sigma}^2$
    
    \section{Analiza wariancji (ANOVA)}
    Rozkład F(-Snedecora): 
    Niech $X$ i $Y$ będą niezależnymi zmiennymi losowymi o rozkładach $\chi_p^2$ i $\chi_q^2$.\\
    Zatem $F=\frac{X/p}{Y/q}$ posiada rozkład F-Snedecora o $(p,q)$ stopniach swobody, jeżeli T jest zmienną losową o rozkładzie $t_q$, to $T^2 \sim F_{1,q}$, $E(F)=\frac{q}{q-2}$ oraz $D^2(F)=\frac{2 q^2 (p+q-2)}{p (q-2)^2 (q-4)}$ dla $q>4$.
    Jednoczynnikowa analiza wariancji:
    Mając $k$ niezależnych próbek prostych: $X_{11},.,X_{1n_1}, X_{21},.,X_{2n_2},...,X_{k1},.,X_{kn_k}$ które pochodzą z $N(m_1,\sigma),..,N(m_k,\sigma)$ testujemy hipotezę: $H_0: m_1=m_2=..=m_k$
    wobec $H_1:$ nie wszystkie wartości $m_i$ są sobie równe. Do weryfikacji $H_0$ służy $f=\frac{MSTR}{MSE}$, $MSTR=\frac{1}{k-1}\Sigma_{i=1}^k n_i(\bar{x_i}-\bar{x})^2$, $MSE=\frac{1}{n-k}\Sigma_{i=1}^k n_i s_i^2$
    $n=\Sigma_{i=1}^k n_i$, $\bar{x_i}$ jest średnią arytmetyczną z i-tej próbki, $s_i^2$ jest wariancją z i-tej próbki, $\bar{x}$ jest średnią arytmetyczną ze wszystkich obserwacji, która daje $F=F(X_{11},..,X_{kn_k})$ o rozkładzie F-Snedecora o $(k-1,n-k)$ stopniach swobody.\\
    \newpage

\end{document}
